{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Processing Pipline\n",
    "Author:\n",
    "- Beitong Tian - beitong2@illinois.edu\n",
    "\n",
    "Nov. 2022\n",
    "\n",
    "Raw Dataset Location: `/data4/dataset`\n",
    "\n",
    "(Please use a test dataset. E.g. copy part of the dataset to another place for experiment)\n",
    "\n",
    "Some Assumptions:\n",
    "1. Sensory data is stored as files in the dataset\n",
    "2. All files have the same format (sampling rate, bit width, ...)\n",
    "\n",
    "Features: \n",
    "1. Read all filepaths\n",
    "2. Print dataset stats (number of files, sampling rate, sample width, duration of each file, etc)\n",
    "3. Check duplicate filenames\n",
    "4. Plot Data Availablity graph (show missing timestamps)\n",
    "5. Iterate Dataset with WindowSize and StepSize\n",
    "6. Extract features of each window\n",
    "7. Train an auto encoder with features\n",
    "8. Test the model on remaining dataset\n",
    "\n",
    "9. Generate and augment anomaly in dataset\n",
    "10. Insert anomalies into the dataset\n",
    "\n",
    "11. Print results of the model on the labeled dataset\n",
    "\n",
    "Visualizations:\n",
    "1. view a window\n",
    "2. view a file\n",
    "3. view files\n",
    "4. view the dataset\n",
    "5. view data around a suspicious anomaly score\n",
    "6. view model performance (confusion matrix, auc plot)\n",
    "7. view anomaly results (anomaly over original graph, paint the abnormal window in light right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from common import *\n",
    "from visualizationCommon import *\n",
    "from feature import *\n",
    "from modelCommon import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please use a test dataset\n",
    "dataDirs = ['/data4/anomalyDetectionTest/experiment3AnomalyDetection'] # the dataset may be stored in multiple dirs, here we use a list to store all dirs\n",
    "windowSize = 48000\n",
    "stepSize = 24000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 1: Read all filepaths\n",
    "1. Filepath is the full path of a file\n",
    "2. Filename is the name of a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepaths = getAllFilePaths(dataDirs)\n",
    "filePathsByday = getAllFilePathByDay(filepaths)\n",
    "printFilePathByDay(filePathsByday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 2: Print dataset/file stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset stats\n",
    "datasetStat = getDatasetStat(filepaths)\n",
    "print(datasetStat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file stats\n",
    "filepath = filepaths[0]\n",
    "fileStat = getFileStat(filepath)\n",
    "print(fileStat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature 3: Check Duplicate Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with MaintletTimer(\"Feature 3: check duplicate files\") as mt:\n",
    "    duplicates, isDuplicate = checkDuplicates(filepaths)\n",
    "    print(\"Duplicate List:\")\n",
    "    for duplicate in duplicates:\n",
    "        print(duplicate)\n",
    "    print(f\"isDuplicate: {isDuplicate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 4: Plot Data Availablity graph (show missing files along the time axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert filepath to filetime\n",
    "# round filetime to seconds\n",
    "# check duplicate\n",
    "x = []\n",
    "for filepath in filepaths:\n",
    "    baseTime = pathToTime(filepath)\n",
    "    for t in range(int(fileStat.duration)):\n",
    "        x.append(baseTime)\n",
    "        baseTime += timedelta(seconds=1)\n",
    "\n",
    "with MaintletTimer(\"Check duplicate file timestamps (the granularity is second)\") as mt:\n",
    "    duplicates, isDuplicate = checkDuplicates(x)\n",
    "    print(\"Duplicate List:\")\n",
    "    for duplicate in duplicates:\n",
    "        print(duplicate)\n",
    "    print(f\"isDuplicate: {isDuplicate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X axis: all timestamps (in seconds) from the beginning to the end of recording\n",
    "# create Y axis: 1 for available, 0 for not available\n",
    "startTime = x[0]\n",
    "endTime = x[-1]\n",
    "X = [pd.to_datetime(d) for d in pd.date_range(start=startTime, end=endTime, freq='S')]\n",
    "Y = []\n",
    "missingTimestamps = []\n",
    "lX = 0\n",
    "lx = 0\n",
    "while lx < len(x) and lX < len(X):\n",
    "    if x[lx].date() == X[lX].date():\n",
    "        Y.append(1)\n",
    "        lx+=1\n",
    "        lX+=1\n",
    "    else:\n",
    "        Y.append(0)\n",
    "        missingTimestamps.append(X[lX])\n",
    "        lX+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(missingTimestamps) > 0:\n",
    "    start = missingTimestamps[0]\n",
    "    end = missingTimestamps[0]\n",
    "    missingTimeRanges = []\n",
    "    for i in range(len(missingTimestamps)-1):\n",
    "        currentT = missingTimestamps[i+1]\n",
    "        previousT = missingTimestamps[i]\n",
    "        deltaT = (currentT - previousT).total_seconds()\n",
    "\n",
    "        if deltaT > 1.0:\n",
    "            missingTimeRanges.append([start, end])\n",
    "            start = currentT\n",
    "        else:\n",
    "            end = currentT\n",
    "    missingTimeRanges.append([start, missingTimestamps[-1]])\n",
    "    print(\"Missing Time Range\")\n",
    "    for missingTimeRange in missingTimeRanges:\n",
    "        print(f\"start {missingTimeRange[0]} end {missingTimeRange[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "f, ax = plt.subplots(figsize=(10, 6))\n",
    "f.suptitle('Data Availablity Plot', fontsize=20)\n",
    "ax.plot(X, Y)\n",
    "ax.set_yticks([0,1])\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Availability\")\n",
    "for label in ax.get_xticklabels(which='major'):\n",
    "    label.set(rotation=30, horizontalalignment='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature5: Iterate Dataset with settings of WindowSize and StepSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepaths = getAllFilePaths(dataDirs)\n",
    "# here we only use the first two files to demo the feature\n",
    "filepaths = filepaths[:2]\n",
    "windowIteratorClass = WindowIterator(filepaths, windowSize, stepSize)\n",
    "windowIter = iter(windowIteratorClass)\n",
    "print(f\"Now Iterating Data Set with Window Size {windowSize} ({windowSize/fileStat.sr} Seconds) and Step Size {stepSize} ({round(stepSize/fileStat.sr,2)} Seconds)\\n\")\n",
    "for window, sampleIndex, windowIndex, time, worldTime in windowIter:\n",
    "    print(f\"Window shape      : {window.shape}\")\n",
    "    print(f\"SampleIndex       : {sampleIndex}\")\n",
    "    print(f\"WindowIndex       : {windowIndex}\")\n",
    "    print(f\"Start Time        : {time}\")\n",
    "    print(f\"Start World Time  : {worldTime[0]}\")\n",
    "    print(f\"End World Time    : {worldTime[1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 1: View Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sensor = 0\n",
    "filepaths = getAllFilePaths(dataDirs)\n",
    "# here we only use the first file to demo the feature\n",
    "filepaths = filepaths[:1]\n",
    "windowIteratorClass = WindowIterator(filepaths, windowSize, stepSize)\n",
    "windowIter = iter(windowIteratorClass)\n",
    "print(f\"Now Iterating Data Set with Window Size {windowSize} ({windowSize/fileStat.sr} Seconds) and Step Size {stepSize} ({round(stepSize/fileStat.sr,2)} Seconds)\\n\")\n",
    "for window, sampleIndex, windowIndex, time, worldTime in windowIter:\n",
    "    visualizeWindow(window[:, sensor])\n",
    "    print(f\"Window shape      : {window.shape}\")\n",
    "    print(f\"SampleIndex       : {sampleIndex}\")\n",
    "    print(f\"WindowIndex       : {windowIndex}\")\n",
    "    print(f\"Start Time        : {time}\")\n",
    "    print(f\"Start World Time  : {worldTime[0]}\")\n",
    "    print(f\"End World Time    : {worldTime[1]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 2-3: View Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "filepaths = getAllFilePaths(dataDirs)\n",
    "# here we only use the first two files to demo the feature\n",
    "filepaths = filepaths[:2]\n",
    "visualizeFiles(filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 4: View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "filepaths = getAllFilePaths(dataDirs)\n",
    "# here we only use the first 10 files to demo the feature\n",
    "filepaths = filepaths[:10]\n",
    "visualizeDataset(filepaths, chunkSize = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 6: Extract features of each window (Log Mel Spectrogram Energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training Dataset: first 100 files\n",
    "filepaths = getAllFilePaths(dataDirs)\n",
    "filepaths = filepaths[:100]\n",
    "\n",
    "# extract features \n",
    "sensorIndex = 0\n",
    "totalWindowCount = getTotalWindowCount(len(filepaths), fileStat.sampleCount, windowSize, stepSize)\n",
    "data = [] # features\n",
    "windows = []\n",
    "n_mels = 64\n",
    "n_frames = 1\n",
    "dims = n_mels * n_frames\n",
    "\n",
    "windowIteratorClass = WindowIterator(filepaths, windowSize, stepSize)\n",
    "windowIter = iter(windowIteratorClass)\n",
    "print(f\"Now Iterating Data Set with Window Size {windowSize} ({windowSize/fileStat.sr} Seconds) and Step Size {stepSize} ({round(stepSize/fileStat.sr,2)} Seconds)\\n\")\n",
    "\n",
    "for window, sampleIndex, windowIndex, _time, worldTime in windowIter:\n",
    "    # get features\n",
    "    vectors = feature1_LogMelEnergies(window[:,sensorIndex])\n",
    "    \n",
    "    # concat features to dataset\n",
    "    if windowIndex == 0:\n",
    "        data = np.zeros((totalWindowCount * vectors.shape[0], dims), float)\n",
    "    data[vectors.shape[0] * windowIndex : vectors.shape[0] * (windowIndex + 1), :] = vectors\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 7: Train an auto encoder with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_model\n",
    "# set some parameters for the model\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "shuffle = True\n",
    "validation_split = 0.1\n",
    "verbose = 1\n",
    "model = keras_model.get_model(n_mels * n_frames, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(x=data,\n",
    "                    y=data,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    validation_split=validation_split,\n",
    "                    verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "saveModel(\"autoencoder\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the train loss\n",
    "loss_plot(history.history[\"loss\"], history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate and visualize the anomaly score in the training dataset\n",
    "# ref: https://github.com/Kota-Dohi/dcase2022_task2_baseline_ae/blob/main/00_train.py\n",
    "y_pred = []\n",
    "start_idx = 0\n",
    "n_vectors_ea_file = int(data.shape[0] / totalWindowCount)\n",
    "for file_idx in range(totalWindowCount):\n",
    "    predict = model.predict(data[start_idx : start_idx + n_vectors_ea_file, :])\n",
    "    \n",
    "    original = data[start_idx : start_idx + n_vectors_ea_file, :]\n",
    "    \n",
    "    # plot spectrogram to visulize the difference between the original and generated spectrogram\n",
    "    # print(file_idx)\n",
    "    # plotSpectrogram(original)\n",
    "    # plotSpectrogram(predict)\n",
    "    \n",
    "    y_pred.append(np.mean(np.square(original - predict)))\n",
    "    start_idx += n_vectors_ea_file\n",
    "    \n",
    "plt.plot(y_pred)\n",
    "plt.title(\"anomaly score in the training dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit anomaly score distribution\n",
    "shape_hat, loc_hat, scale_hat = fitAnomalyScore(y_pred)\n",
    "# calculate anomaly score threshold\n",
    "decision_threshold = scipy.stats.gamma.ppf(q=0.9, a=shape_hat, loc=loc_hat, scale=scale_hat)\n",
    "print(f\"Anomaly Threshold: {decision_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 8: Test the model on remaining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "# the flag for enabling the animation\n",
    "isAnimate = False\n",
    "\n",
    "# test dataset: here we use the following 100 files \n",
    "filepaths = getAllFilePaths(dataDirs)\n",
    "filepaths = filepaths[100:200]\n",
    "\n",
    "# setup parameters\n",
    "sensorIndex = 0\n",
    "newPreds = []\n",
    "avgTime = 0\n",
    "counter = 0\n",
    "plotSampleCount = 10000\n",
    "windowTimeBuffer = [] # record the world time for each window\n",
    "x = [i-plotSampleCount for i in range(plotSampleCount)]\n",
    "y = []\n",
    "\n",
    "# animation\n",
    "plt.figure(figsize=(18, 6))\n",
    "def animate(newData, startIndex):\n",
    "    global line, x, y, plotSampleCount\n",
    "    x = x[1:]\n",
    "    x.append(startIndex)\n",
    "    if len(y) == 0:\n",
    "        y = [newData for i in range(plotSampleCount)]\n",
    "    else:\n",
    "        y = y[1:]\n",
    "        y = np.append(y, newData)\n",
    "    # line.set_xdata(x)\n",
    "    # line.set_ydata(y)  # update the data\n",
    "    plt.gca().cla() \n",
    "    plt.plot(x,y)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "\n",
    "# iterate over test dataset and get predicts\n",
    "totalWindowEstimate =  getTotalWindowCount(len(filepaths), fileStat.sampleCount, windowSize, stepSize)\n",
    "windowIteratorClass = WindowIterator(filepaths, windowSize, stepSize)\n",
    "windowIter = iter(windowIteratorClass)\n",
    "print(f\"Now Iterating Data Set with Window Size {windowSize} ({windowSize/fileStat.sr} Seconds) and Step Size {stepSize} ({round(stepSize/fileStat.sr,2)} Seconds)\\n\")\n",
    "for window, sampleIndex, windowIndex, timeIndex, worldTime in windowIter:\n",
    "    windowTimeBuffer.append(worldTime)\n",
    "    startTime = time.time()\n",
    "    # window shape: sample x channel\n",
    "    vectors = feature1_LogMelEnergies(window[:,sensorIndex])\n",
    "    predict = model.predict(vectors)\n",
    "    \n",
    "    original = vectors\n",
    "    # print(file_idx)\n",
    "    # plotSpectrogram(original)\n",
    "    # plotSpectrogram(predict)\n",
    "    newAbnormalScore = np.mean(np.square(original - predict))\n",
    "    if isAnimate:\n",
    "        animate(newAbnormalScore, windowIndex)\n",
    "    # append abnormal scores\n",
    "    newPreds.append(newAbnormalScore)\n",
    "    \n",
    "    # make decision (TODO 11072022)\n",
    "    # idea1: just compare the abnormal score with the threshold (either set the p to 0.999 which is nonsense, or generate lots of FPs)\n",
    "    # idea2: build a probability model to reduce the FP\n",
    "    # need to read more papers\n",
    "    \n",
    "    \n",
    "    # calculate inference time\n",
    "    elapsedTime = time.time() - startTime\n",
    "    if avgTime == 0:\n",
    "        avgTime = elapsedTime\n",
    "    else:\n",
    "        avgTime = (avgTime*counter + elapsedTime) / (counter+1)\n",
    "    counter += 1\n",
    "    print(f\"\\r\\ravgTime {round(avgTime*1000, 4): <10} ms | window: {windowIndex: < 10} out of {totalWindowEstimate: < 10} | Remaining Time {round((totalWindowEstimate - windowIndex)*avgTime, 2)} Seconds\", end=\"\",flush=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot anomaly scroe results\n",
    "%matplotlib widget\n",
    "plt.plot(newPreds)\n",
    "plt.title(\"anomaly score from test dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 5: view data around an interesting anomaly score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: zoom in the plot of anomaly score from test dataset (the plot in the above cell)\n",
    "# step 2: record the index\n",
    "# step 3: change the interesting Index below to the index you find in the step 2\n",
    "%matplotlib inline\n",
    "interestingIndex = 1880 \n",
    "checkDataAroundTimeWindow(windowTimeBuffer[interestingIndex], filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 11: Print performance of the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we generate the ground truth and the anomaly detection result\n",
    "# we consider two kinds of results\n",
    "# 1. window-wise (1S)\n",
    "# 2. file-wise (10S)\n",
    "# also, we need to think about the correct metrics for our problem\n",
    "# 1. we want to know the start and end of the anomaly so we can clip it. \n",
    "# 2. another way is we can choose a long enough window\n",
    "# is it useful to know a 1 second window is abnormal is useful? \n",
    "# we can send a longer suspicious window to the expert and then clip afterwards\n",
    "\n",
    "# for simulating groundtruth\n",
    "testFileCount = 100\n",
    "filepaths = range(testFileCount)\n",
    "totalWindowCount = getTotalWindowCount(len(filepaths), fileStat.sampleCount, windowSize, stepSize)\n",
    "\n",
    "samples = stats.gamma.rvs(a=shape_hat, scale=scale_hat, loc=loc_hat, size = totalWindowCount) # sample pred from gamma distribution\n",
    "\n",
    "decisionThresholdForGroundTruth = scipy.stats.gamma.ppf(q=0.91, a=shape_hat, loc=loc_hat, scale=scale_hat)\n",
    "decisionThresholdForDecision = scipy.stats.gamma.ppf(q=0.9, a=shape_hat, loc=loc_hat, scale=scale_hat)\n",
    "\n",
    "# replace the following three lines based on the output of your model\n",
    "yTrue = [1 if i > decisionThresholdForGroundTruth else 0 for i in samples]\n",
    "yPred = samples # pred score\n",
    "decision = [1 if i > decisionThresholdForDecision else 0 for i in samples]\n",
    "\n",
    "max_fpr = 0.1 # for pAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[auc, p_auc, prec, recall, f1] = getModelPerformance(yTrue, yPred, decision, max_fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "csvLines = []\n",
    "csvLines.append([\"model\", \"AUC\", \"pAUC\", \"precision\", \"recall\", \"F1 score\"])\n",
    "csvLines.append([\"autoencoder\", auc, p_auc, prec, recall, f1])\n",
    "saveCSV(\"modelRes\", csvLines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 6: view model performance (confusion matrix, ROC plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "rocPlot(yTrue, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusionMatrixPlot(yTrue, decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are some functions only for this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get world time of all windows from file #100\n",
    "filepaths = getAllFilePaths(dataDirs)[100:]\n",
    "totalWindowEstimate =  int((len(filepaths)* fileStat.sampleCount - windowSize)/stepSize) + 1\n",
    "data = []\n",
    "windows = []\n",
    "worldTimes = []\n",
    "n_mels = 64\n",
    "n_frames = 1\n",
    "dims = n_mels * n_frames\n",
    "windowIteratorClass = WindowIterator(filepaths, windowSize, stepSize)\n",
    "windowIter = iter(windowIteratorClass)\n",
    "print(f\"Now Iterating Data Set with Window Size {windowSize} ({windowSize/fileStat.sr} Seconds) and Step Size {stepSize} ({round(stepSize/fileStat.sr,2)} Seconds)\\n\")\n",
    "for window, sampleIndex, windowIndex, time, worldTime in windowIter:\n",
    "    # window shape: sample x channel\n",
    "    # windows.append(window)\n",
    "    worldTimes.append(worldTime)\n",
    "    if windowIndex % 100 == 0:\n",
    "        print(f\"\\rwindow: {windowIndex: < 10} out of {totalWindowEstimate: < 10}\", end=\"\",flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load history data\n",
    "newPreds = loadFile('2022-11-06-16-25-32-newPreds100end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(newPreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = getAllFilePaths(dataDirs)[100:]\n",
    "len(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalWindowEstimate =  int((len(filepaths)* fileStat.sampleCount - windowSize)/stepSize) + 1\n",
    "totalWindowEstimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predForEachFile = chunks(newPreds, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "yPredForEachFile = []\n",
    "for i in predForEachFile:\n",
    "    yPredForEachFile.append(np.average(i))\n",
    "print(len(yPredForEachFile))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(yPredForEachFile)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "visualizeFiles(filepaths[28600:28800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeFiles(filepaths[21063:21067], play=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeFiles(filepaths[15195:15200], play=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
